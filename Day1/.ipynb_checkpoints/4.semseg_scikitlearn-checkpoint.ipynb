{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic segmentation with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Written by Dr Daniel Buscombe, Northern Arizona University\n",
    "\n",
    "> Part of a series of notebooks for image recognition and classification using deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates some strategies for semantic image segmentation using common machine learning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem. This section will focus on an intuitive explanation of how naive Bayes classifiers work, followed by a couple examples of them in action on some datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of sand using Naïve Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, Naïve Bayes classification is employed to detect pixels corresponding to sand in images, based just in the pixels color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is a M×N×3 array representing a color training image, and mask a M×N binary array representing the classification sand/non-sand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read an image in and look at its distributions of red, green and blue values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open('cdi-workshop/semseg_data/sandbars/RC0307Rf_20131111_1347.JPG', 'rb') as f:\n",
    "    training_rgb = imread(f)\n",
    "M, N, _ = training_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,255,30)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "hist = plt.hist(training_rgb[:,:,0].flatten(), bins=bins, color='r', alpha=0.5)\n",
    "hist = plt.hist(training_rgb[:,:,1].flatten(), bins=bins, color='g', alpha=0.5)\n",
    "hist = plt.hist(training_rgb[:,:,2].flatten(), bins=bins, color='b', alpha=0.5)\n",
    "plt.title('Pixel values', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An (overly) simplistic approach to finding sand in a given image would be to find some threshold intensity in a certain channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 200\n",
    "\n",
    "mask = np.zeros((M,N))\n",
    "mask[training_rgb[:,:,0] > threshold] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(training_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is composed by MN 3d-vectors of red, green and blue values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(training_rgb))\n",
    "data = training_rgb.reshape(M*N, -1)[:,:]\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification used in the learning step is represented as a binary MN vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mask.reshape(M*N)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides a naive_bayes module containing a GaussianNB object that implements the supervised learning by the Gaussian Naïve Bayes method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions. This model can be fit by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sand detection can be performed by reshaping and slicing in the same way as the training image. \n",
    "\n",
    "The predict method of GaussianNB performs the classification. The resulting classification vector can be reshaped to the original image dimensions for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test at an image from the same place but at a different time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open('cdi-workshop/semseg_data/sandbars/RC0307Rf_20161106_1157.JPG', 'rb') as f:\n",
    "    test_rgb = imread(f)\n",
    "M_tst, N_tst, _ = test_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_rgb.reshape(M_tst * N_tst, -1)[:,:]\n",
    "sand_pred = gnb.predict(data)\n",
    "S = sand_pred.reshape(M_tst, N_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(S, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(S, cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the probabilities of each class because we have a simple recipe to compute the likelihood $P({\\rm features}~|~L_1)$ for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sand_pred = gnb.predict_proba(data)\n",
    "Sprob = sand_pred.reshape(M_tst, N_tst, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Sprob[:,:,1], cmap=plt.cm.bwr)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.25)\n",
    "plt.title('Probability of Sand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(S, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(Sprob[:,:,1]>.99, cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the classifier from an image taken at a different place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open('cdi-workshop/semseg_data/sandbars/RC0220Ra_20150219_1126.JPG', 'rb') as f:\n",
    "    test_rgb = imread(f)\n",
    "\n",
    "M_tst, N_tst, _ = test_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_rgb.reshape(M_tst * N_tst, -1)[:,:]\n",
    "sand_pred = gnb.predict(data)\n",
    "S = sand_pred.reshape(M_tst, N_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(S, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(S, cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great. Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make things more complicated by\n",
    "* adding more classes\n",
    "* using feature extraction\n",
    "\n",
    "Unlike before, we can build the feature extraction straight into the model using pipelines, which sequentially apply a list of transforms and a final estimator. In our case we'll use PCA as a transform again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(svd_solver='randomized', n_components=3, whiten=True, random_state=42)\n",
    "model = make_pipeline(pca, gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbitrary thresholds can be made to make classes based on intensity alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_rgb.reshape(M*N, -1)[:,:]\n",
    "\n",
    "threshold_sand = 200\n",
    "threshold_shadow = 60\n",
    "threshold_rock = 100\n",
    "\n",
    "mask = np.zeros((M,N))\n",
    "mask[training_rgb[:,:,0] > threshold_sand] = 3\n",
    "mask[training_rgb[:,:,0] < threshold_shadow] = 0\n",
    "mask[(training_rgb[:,:,0] > threshold_rock) & (training_rgb[:,:,0] < threshold_sand) ] = 1\n",
    "mask[(training_rgb[:,:,0] > threshold_shadow) & (training_rgb[:,:,0] < threshold_rock) ] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(training_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mask, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = mask.reshape(M*N)\n",
    "model.fit(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model to the test image and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_rgb.reshape(M_tst * N_tst, -1)[:,:]\n",
    "sand_pred = model.predict(data)\n",
    "S = sand_pred.reshape(M_tst, N_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(S) #, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.1)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(S==3, cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there wasn't much advantage using feature extraction with the NB model. Let's look at a different model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying thresholds is a big weakness. There are other approaches that attempt to estimate the decision boundaries between different classes. One example is a Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use downscaled versions of images to speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imresize\n",
    "with fs.open('cdi-workshop/semseg_data/sandbars/RC0307Rf_20131111_1347.JPG', 'rb') as f:\n",
    "    training_rgb = imresize(imread(f), .125)\n",
    "    \n",
    "with fs.open('cdi-workshop/semseg_data/sandbars/RC0220Ra_20150219_1126.JPG', 'rb') as f:\n",
    "    test_rgb = imresize(imread(f), .125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with 4 components to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, _ = training_rgb.shape\n",
    "data = training_rgb.reshape(M*N, -1)[:,:]\n",
    "gmm = GaussianMixture(n_components=4, covariance_type=\"tied\").fit(data)\n",
    "labels = gmm.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMMs use an expectation–maximization approach which qualitatively does the following:\n",
    "\n",
    "Choose starting guesses for the location and shape\n",
    "\n",
    "Repeat until converged:\n",
    "\n",
    "* E-step: for each point, find weights encoding the probability of membership in each cluster\n",
    "* M-step: for each cluster, update its location, normalization, and shape based on all data points, making use of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only show every 10th data point to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[::10, 0], data[::10, 1], c=labels[::10], s=5, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to the test image and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = test_rgb.reshape(M * N, -1)[:,:]\n",
    "cluster = gmm.predict(newdata)\n",
    "cluster = cluster.reshape(M, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(cluster) #, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.1)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow(cluster==3, cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn this is done using the predict_proba method. This returns a matrix of size [n_samples, n_clusters] which measures the probability that any point belongs to the given cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_probs = gmm.predict_proba(newdata)\n",
    "np.shape(post_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(post_probs[:,3].reshape(M, N), cmap=plt.cm.bwr)\n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.title('Probability of Sand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset. A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, using cross-validation to avoid over-fitting. \n",
    "\n",
    "Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC). Scikit-Learn's GMM estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(2, 15)\n",
    "models = [GaussianMixture(n, covariance_type='tied', random_state=0).fit(data)\n",
    "          for n in n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_components, [m.bic(data) for m in models], 'k--o', label='BIC')\n",
    "plt.plot(n_components, [m.aic(data) for m in models], 'r-s', label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, AIC and BIC are the same\n",
    "\n",
    "The optimal number of clusters is the value that minimizes the AIC or BIC. \n",
    "\n",
    "It says about 9 components would have been a better choice than 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=9, covariance_type=\"tied\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = test_rgb.reshape(M * N, -1)[:,:]\n",
    "cluster = gmm.predict(newdata)\n",
    "cluster = cluster.reshape(M, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the means for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm.means_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm.means_[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(cluster) \n",
    "plt.axis('off')\n",
    "plt.colorbar(shrink=0.15)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(test_rgb, alpha=0.6)\n",
    "plt.imshow((cluster==2)  + (cluster==5), cmap=plt.cm.binary_r, alpha=0.4)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like cluster 2 represents lower beach and cluster 5 represents upper beach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What choices did we make to arrive at this result?\n",
    "* which cluster corresponds to what feature\n",
    "* number of components\n",
    "* type of covariance\n",
    "\n",
    "How well does this approach generalize to all sandbar images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
