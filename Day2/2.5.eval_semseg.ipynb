{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating semantic segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Written by Dr Daniel Buscombe, Northern Arizona University\n",
    "\n",
    "> Part of a series of notebooks for image recognition and classification using deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard metrics of precision, P, recall, R, accuracy, A, and F1 score, F, are used to assess classification of image regions and pixels. \n",
    "\n",
    "Where TP, TN, FP, and FN are, respectively, the frequencies of true positives, true negatives, false positives, and false negatives: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P=  \\frac{TP}{(TP+FP)}$\n",
    "\n",
    "$R=  \\frac{TP}{(TP+FN)}$\n",
    "\n",
    "$A=  \\frac{(TP+TN)}{(TP+TN+FP+FN)}$\n",
    "\n",
    "$F=2\\times \\frac{(P \\times R)}{(P+R)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives are image regions/pixels correctly classified as belonging to a certain class by the model, while true negatives are correctly classified as not belonging to a certain class. \n",
    "\n",
    "False negatives are regions/pixels incorrectly classified as not belonging to a certain class, and false positives are those regions/pixels incorrectly classified as belonging to a certain class. \n",
    "\n",
    "Precision and recall are useful where the number of observations belonging to one class is significantly lower than those belonging to the other classes. \n",
    "\n",
    "Recall is a measure of the ability to detect the occurrence of a class, which is a given landform, land use or land cover.\n",
    " \n",
    "These metrics are therefore used in evaluation of pixelwise segmentations, where the number of pixels corresponding to each class vary considerably. \n",
    "\n",
    "The F1 score is an equal weighting of the recall and precision and quantifies how well the model performs in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time\n",
    "from glob import glob\n",
    "from imageio import imread\n",
    "import itertools\n",
    "\n",
    "#numerical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#supress tf warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# suppress divide and invalid warnings\n",
    "np.seterr(divide='ignore')\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "def load_graph(model_file):\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# =========================================================\n",
    "def getCP(tmp, graph):  \n",
    "    #graph = load_graph(classifier_file)\n",
    "\n",
    "    input_name = \"import/Placeholder\" ##+ input_layer\n",
    "    output_name = \"import/final_result\" ##+ output_layer\n",
    "    input_operation = graph.get_operation_by_name(input_name);\n",
    "    output_operation = graph.get_operation_by_name(output_name);\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        results = sess.run(output_operation.outputs[0],\n",
    "                      {input_operation.outputs[0]: np.expand_dims(tmp, axis=0)})\n",
    "    results = np.squeeze(results)\n",
    "\n",
    "    # Sort to show labels of first prediction in order of confidence\n",
    "    top_k = results.argsort()[-len(results):][::-1]\n",
    "\n",
    "    return top_k[0], results[top_k[0]], results[top_k] #, np.std(tmp[:,:,0])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "def norm_im(image_path):\n",
    "    input_mean = 0 #128\n",
    "    input_std = 255 #128\n",
    "\n",
    "    input_name = \"file_reader\"\n",
    "    output_name = \"normalized\"\n",
    "    img = imread(image_path)\n",
    "    nx, ny, nz = np.shape(img)\n",
    "\n",
    "    theta = np.std(img).astype('int')\n",
    "\n",
    "    file_reader = tf.read_file(image_path, input_name)\n",
    "    image_reader = tf.image.decode_jpeg(file_reader, channels = 3,\n",
    "                                        name='jpeg_reader')\n",
    "    float_caster = tf.cast(image_reader, tf.float32)\n",
    "\n",
    "    dims_expander = tf.expand_dims(float_caster, 0);\n",
    "    normalized = tf.divide(tf.subtract(dims_expander, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    return np.squeeze(sess.run(normalized))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tiles(label, direc, numero, classifier_file, x, n):\n",
    "    #for label in labels:\n",
    "    graph = load_graph(classifier_file)\n",
    "\n",
    "    print(label)\n",
    "    infiles = glob(direc+os.sep+label+os.sep+'*.jpg')[:numero]\n",
    "\n",
    "    Z = []\n",
    "    for image_path in infiles:\n",
    "        Z.append(norm_im(image_path))\n",
    "\n",
    "    w1 = []\n",
    "    for i in range(len(Z)):\n",
    "        w1.append(getCP(Z[i], graph))\n",
    "\n",
    "    C, P, _ = zip(*w1) ##,S\n",
    "    del w1, Z\n",
    "\n",
    "    C = np.asarray(C)\n",
    "    P = np.asarray(P)\n",
    "\n",
    "    e = precision_recall_fscore_support(np.ones(len(C))*x, C)\n",
    "\n",
    "    cm = np.zeros((n,n))\n",
    "    for a, p in zip(np.ones(len(C), dtype='int')*x, C):\n",
    "        cm[a][p] += 1\n",
    "\n",
    "    cm = cm[x,:]\n",
    "\n",
    "    p = np.max(e[0])\n",
    "    r = np.max(e[1])\n",
    "    f = np.max(e[2])\n",
    "    a = np.sum([c==x for c in C])/len(C)\n",
    "    #print(label+' accuracy %f' % (a))\n",
    "    #print('precision %f' % (p) )\n",
    "    #print('recall %f' % (r) )\n",
    "    #print('f score %f' % (f) )\n",
    "    #print('mean prob. %f' % (np.mean(P)) )\n",
    "    return [a,p,r,f, np.mean(P)], cm ##C,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = 'labels.txt'\n",
    "classifier_file = 'monterey_mobilenetv2_96_1000_0.01.pb'\n",
    "\n",
    "## number of tiles to evaluate\n",
    "numero = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads label file, strips off carriage return\n",
    "labels = [line.rstrip() for line \n",
    "             in tf.gfile.GFile(class_file)]\n",
    "code= {}\n",
    "for label in labels:\n",
    "    code[label] = [i for i, x in enumerate([x.startswith(label) for x in labels]) if x].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = 'train/tile_96'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=[]\n",
    "for label in labels:\n",
    "    w.append(eval_tiles(label, direc, numero, classifier_file, code[label], len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, CM = zip(*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.asarray(E)[:,0] \n",
    "f= np.asarray(E)[:,3] \n",
    "pr= np.asarray(E)[:,4] \n",
    "\n",
    "print('mean accuracy. %f' % (np.mean(a)) )\n",
    "print('mean Fscore. %f' % (np.mean(f)) )\n",
    "print('mean prob. %f' % (np.mean(pr)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ‘confusion matrix’, which is the matrix of normalized correspondences between true and estimated labels, is a convenient way to visualize model skill. \n",
    "\n",
    "A perfect correspondence between true and estimated labels is scored 1.0 along the diagonal elements of the matrix.\n",
    "\n",
    "Misclassiﬁcations are readily identiﬁed as off-diagonal elements. Systematic misclassiﬁcations are recognized as off-diagonal elements with large magnitudes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## =========================================================\n",
    "def plot_confusion_matrix2(cm, classes, normalize=False, cmap=plt.cm.Blues, dolabels=True):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm[np.isnan(cm)] = 0\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmax=1, vmin=0)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    if dolabels==True:\n",
    "       tick_marks = np.arange(len(classes))\n",
    "       plt.xticks(tick_marks, classes, fontsize=3) # rotation=45\n",
    "       plt.yticks(tick_marks, classes, fontsize=3)\n",
    "\n",
    "       #plt.ylabel('True label',fontsize=6)\n",
    "       #plt.xlabel('Estimated label',fontsize=6)\n",
    "\n",
    "    else:\n",
    "       plt.axis('off')\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if cm[i, j]>0:\n",
    "           plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 fontsize=3,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = np.asarray(CM)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(221)\n",
    "plot_confusion_matrix2(CM, classes=labels, normalize=True, cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
